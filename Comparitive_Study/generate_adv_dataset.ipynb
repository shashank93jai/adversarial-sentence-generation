{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fba03573050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "from bleu import compute_bleu\n",
    "from models import load_models, generate\n",
    "from utils import batchify, to_gpu\n",
    "from utils import Corpus, filter_flip_polarity\n",
    "random.seed(1111)\n",
    "np.random.seed(1111)\n",
    "torch.manual_seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './output/hsieh_bpe_20_epochs'\n",
    "DATA_DIR = './data/hsieh_bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from./output/hsieh_bpe_20_epochs\n"
     ]
    }
   ],
   "source": [
    "model_args, idx2word, autoencoder, gan_gen, gan_disc, enc_classifier \\\n",
    "        = load_models(MODEL_DIR, suffix=\"_10\", on_gpu=True, arch_cl=\"100\")\n",
    "\n",
    "# not needed\n",
    "del gan_gen\n",
    "del gan_disc\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "word2idx = json.load(open(\"{}/vocab.json\".format(MODEL_DIR), \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab file ./output/hsieh_bpe_20_epochs/vocab.json with 5971 words\n",
      "Number of sentences cropped from ./data/hsieh_bpe/train.txt: 0 out of 100000 total, dropped 1517. OOV rate 0.000\n",
      "Using test.txt as test set\n",
      "Number of sentences cropped from ./data/hsieh_bpe/test.txt: 0 out of 100000 total, dropped 1538. OOV rate 0.000\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(DATA_DIR,\n",
    "                maxlen=30,\n",
    "                vocab_size=12000,\n",
    "                lowercase=False,\n",
    "                max_lines=100000,\n",
    "                test_size=-1,\n",
    "                load_vocab_file=os.path.join(MODEL_DIR, 'vocab.json'),\n",
    "                test_path='test.txt',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Do we need to flip the labels? or is that because they do FGSM as minus eps x grad?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 35\n",
    "# f_test = filter_flip_polarity(corpus.test)\n",
    "# test_data = batchify(f_test, bsz=bsz, shuffle=False, pad_id=0)\n",
    "\n",
    "test_data = batchify(corpus.test, bsz=bsz, shuffle=False, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_original_sentence(source_batch, bpe_decode=False):\n",
    "    # recover original sentence\n",
    "    original_sentences = []\n",
    "    indices = source_batch.numpy()\n",
    "    for idx in indices:\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx if x > 1]\n",
    "        \n",
    "        if bpe_decode:\n",
    "            # if tokens were encoded using bpemb_en, then we can convert the tokens back to English words\n",
    "            original_sentences.append(bpemb_en.decode(words))\n",
    "        else:\n",
    "            # leave as BPE tokens\n",
    "            original_sentences.append(\" \".join(words))\n",
    "    return original_sentences\n",
    "\n",
    "def generate_sentences_from_embedding(autoencoder, embedded_sentences_batch, sample=False, bpe_decode=False):\n",
    "    # generate sentence from embedding\n",
    "    decoded_sentences = []\n",
    "    max_indices = autoencoder.generate(embedded_sentences_batch, maxlen=50, sample=sample)\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        if bpe_decode:\n",
    "            # if tokens were encoded using bpemb_en, then we can convert the tokens back to English words\n",
    "            decoded_sentences.append(bpemb_en.decode(truncated_sent))\n",
    "        else:\n",
    "            # leave as BPE tokens\n",
    "            decoded_sentences.append(\" \".join(truncated_sent))  \n",
    "    return decoded_sentences\n",
    "\n",
    "def fgsm_attack(sentence_embedding, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_embedding = sentence_embedding + epsilon*sign_data_grad\n",
    "    #clip within normal range for embedding\n",
    "    perturbed_embedding = torch.clamp(perturbed_embedding, -0.34, 0.32)\n",
    "    return perturbed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "\n",
    "def generate_adversarial_dataset(data,\n",
    "                                 directory='./adversarial_data',\n",
    "                                 phase='test',  # for naming the output files appropriately\n",
    "                                 perturb='pgd',\n",
    "                                 epsilon=.015,\n",
    "                                 alpha=.015, pgd_iters=40):\n",
    "\n",
    "    assert perturb in [\"fgsm\", \"pgd\"], \"perturb should be 'fgsm' or 'pgd'\"\n",
    "    all_tags = []\n",
    "    predicted_tags = []\n",
    "    original_sentences = []\n",
    "    source_decoded = []\n",
    "    adv_decoded = []\n",
    "    \n",
    "    print()\n",
    "    print(\"Dataset:\", phase)\n",
    "    if perturb == 'fgsm':\n",
    "        print(\"FGSM with epsilon {}.\".format(epsilon))\n",
    "    elif perturb == 'pgd':\n",
    "        print(\"PGD with epsilon {} and alpha {}, {} iters.\".format(epsilon, alpha, pgd_iters))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in data:\n",
    "        enc_classifier.zero_grad()\n",
    "        autoencoder.zero_grad()\n",
    "        source, target, lengths, tags = batch\n",
    "        source = to_gpu(True, Variable(source))\n",
    "        #target = to_gpu(True, Variable(target)) # word ID\n",
    "        tags = to_gpu(True, Variable(tags))\n",
    "\n",
    "        # recover original sentence as BPE tokens\n",
    "        original_sentences.extend(recover_original_sentence(source.cpu()))\n",
    "\n",
    "        # keep flat list of all tags\n",
    "        all_tags.extend(tags.cpu().numpy())\n",
    "\n",
    "        # autoencoder encoded\n",
    "        output_encode_only = autoencoder(source, lengths, noise=False, encode_only=True)\n",
    "        output_encode_only.retain_grad()  # NL: same as output_encode_only.requires_grad = True\n",
    "\n",
    "        # classifier output\n",
    "        output_classifier = enc_classifier(output_encode_only)\n",
    "\n",
    "        # keep the predicted label\n",
    "        _, output_classifier_argmax = torch.max(output_classifier, -1)\n",
    "        predicted_tags.extend(output_classifier_argmax.cpu().numpy())\n",
    "\n",
    "        # apply perturbation\n",
    "        if perturb == 'fgsm':\n",
    "            classifier_loss = criterion_ce(output_classifier, tags)\n",
    "            enc_classifier.zero_grad()\n",
    "            classifier_loss.backward()\n",
    "            code_grad = output_encode_only.grad.data\n",
    "            perturbed_code = fgsm_attack(output_encode_only, epsilon, code_grad)        \n",
    "\n",
    "        elif perturb == 'pgd':\n",
    "            # alpha: step size\n",
    "            # epsilon: max perturbation (ball)\n",
    "            perturbed_code = output_encode_only.clone().detach()\n",
    "            for i in range(pgd_iters):\n",
    "                perturbed_code.requires_grad = True\n",
    "                scores = enc_classifier(perturbed_code)\n",
    "                tmp_loss = criterion_ce(scores, tags)\n",
    "                enc_classifier.zero_grad()\n",
    "                tmp_loss.backward(retain_graph=True)\n",
    "\n",
    "                # step in the direction of the gradient\n",
    "                perturbed_code = perturbed_code + alpha * perturbed_code.grad.sign()\n",
    "\n",
    "                # Workaround as PyTorch doesn't have elementwise clip\n",
    "                # from: https://gist.github.com/oscarknagg/45b187c236c6262b1c4bbe2d0920ded6#file-projected_gradient_descent-py\n",
    "                perturbed_code = torch.max(torch.min(perturbed_code, code + epsilon), code - epsilon).detach()\n",
    "                perturbed_code = torch.clamp(perturbed_code, -0.34, 0.32)\n",
    "\n",
    "        # decode perturbed sentence\n",
    "        adv_decoded.extend(generate_sentences_from_embedding(autoencoder, perturbed_code))\n",
    "\n",
    "        # decode original sentence, for comparison\n",
    "        source_decoded.extend(generate_sentences_from_embedding(autoencoder, output_encode_only))\n",
    "\n",
    "    # write data\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    # Create new datasets of decoded adversarial sentences, but only those that \n",
    "    # do not exactly match their unperturbed counterpart after both are decoded,\n",
    "    # and those that were initially classified correctly\n",
    "    if perturb == 'fgsm':\n",
    "        target_text_file = os.path.join(directory, 'fgsm eps_{} {}.txt'.format(epsilon, phase))\n",
    "        target_df_file = os.path.join(directory, 'fgsm eps_{} {}.csv'.format(epsilon, phase))\n",
    "    elif perturb == 'pgd':\n",
    "        target_text_file = os.path.join(directory, 'pgd eps_{} alpha_{} iters_{} {}.txt'.format(epsilon, alpha, pgd_iters, phase))\n",
    "        target_df_file = os.path.join(directory, 'pgd eps_{} alpha_{} iters_{} {}.csv'.format(epsilon, alpha, pgd_iters, phase))\n",
    "\n",
    "    with open(target_text_file, 'w') as f:\n",
    "        for s, d, t, p in zip(source_decoded, adv_decoded, all_tags, predicted_tags):\n",
    "            if (s != d) and (p == t):\n",
    "                # only keep if sentence was originally classified correctly,\n",
    "                # and if the perturbed embedding is different than the original embedding\n",
    "                # after they've both been decoded\n",
    "                f.write(str(t))\n",
    "                f.write(\"\\t\")\n",
    "                f.write(d)\n",
    "                f.write(\"\\n\")\n",
    "    print(\"Done writing adversarial data to\", target_text_file)\n",
    "\n",
    "    # For inspection, write all of the data to file\n",
    "    df = pd.DataFrame(list(zip(original_sentences,\n",
    "                               adv_decoded,\n",
    "                               source_decoded,\n",
    "                               all_tags,\n",
    "                               predicted_tags)), \n",
    "                   columns =['original', 'adv_decoded', 'original_decoded', 'label', 'predicted']) \n",
    "    df['included_in_adv_dataset'] = (df['adv_decoded'] != df['original_decoded']) & (df['label'] == df['predicted'])\n",
    "    df.to_csv(target_df_file)\n",
    "    print(\"Done writing dataframe to\", target_df_file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.001.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.001 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.001 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.006.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.006 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.006 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.011.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.011 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.011 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.016.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.016 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.016 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.021.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.021 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.021 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.026.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.026 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.026 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.031.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.031 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.031 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.036.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.036 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.036 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.041.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.041 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.041 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.046.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.046 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.046 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.051.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.051 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.051 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.056.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.056 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.056 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.061.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.061 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.061 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.066.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.066 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.066 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.071.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.071 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.071 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.076.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.076 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.076 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.081.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.081 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.081 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.086.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.086 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.086 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.091.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.091 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.091 test.csv\n",
      "\n",
      "\n",
      "Dataset: test\n",
      "FGSM with epsilon 0.096.\n",
      "Done writing adversarial data to ./adversarial_data/fgsm eps_0.096 test.txt\n",
      "Done writing dataframe to ./adversarial_data/fgsm eps_0.096 test.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eps_range = [np.round(x, 3) for x in np.arange(1e-3, 1e-1, 5e-3)]\n",
    "\n",
    "for epsilon in eps_range:\n",
    "    generate_adversarial_dataset(test_data, perturb='fgsm', epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
