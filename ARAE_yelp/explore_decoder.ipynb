{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify\n",
    "from models import Seq2Seq2Decoder, Seq2Seq, MLP_D, MLP_G, MLP_Classify\n",
    "import shutil\n",
    "\n",
    "from bpemb import BPEmb  # for decoding BPE tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code global variables that were command line arguments in the training script\n",
    "OUTF = \"yelp_example\"\n",
    "MODELF = \"model_output_128_128_128_NoBatchNorm\"\n",
    "VOCAB_SIZE = 8000\n",
    "MAXLEN = 30\n",
    "LOWERCASE = True\n",
    "EMSIZE = 300\n",
    "NHIDDEN = 512\n",
    "BATCH_SIZE = 64\n",
    "NLAYERS = 1\n",
    "NOISE_R = 0.1\n",
    "DROPOUT = 0.0\n",
    "HIDDEN_INIT = True\n",
    "Z_SIZE = 32\n",
    "ARCH_CLASSIFY = '128-128-128' \n",
    "ARCH_D = '128-128'\n",
    "ARCH_G = '128-128'\n",
    "LR_AE = 1\n",
    "LR_GAN_G = 1e-04\n",
    "LR_GAN_D = 1e-04\n",
    "LR_CLASSIFY = 1e-04\n",
    "BETA1 = 0.4\n",
    "CUDA = True\n",
    "SEED = 1111\n",
    "LAMBDA_CLASS = 1\n",
    "CLIP = 1\n",
    "TEMP = 1\n",
    "LOG_INTERVAL = 200\n",
    "NITERS_GAN_SCHEDULE = ''\n",
    "GRAD_LAMBDA = 0.01\n",
    "GAN_GP_LAMBDA = 0.1\n",
    "EPOCHS = 0\n",
    "NITERS_AE = 1\n",
    "NITERS_GAN_G = 1\n",
    "NITERS_GAN_D = 5\n",
    "NITERS_GAN_AE = 1\n",
    "NOISE_ANNEAL = 0.9995\n",
    "DATA_PATH = './data/'\n",
    "ADVERSARIAL_DATA_PATH = './adversarial_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# make output directory if it doesn't already exist\n",
    "if not os.path.isdir(OUTF):\n",
    "    os.makedirs(OUTF)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    if not CUDA:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(DATA_PATH, \"test1.txt\"), \"test1\", False),\n",
    "             (os.path.join(DATA_PATH, \"test0.txt\"), \"test2\", False),\n",
    "             (os.path.join(DATA_PATH, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(DATA_PATH, \"train2.txt\"), \"train2\", True)]\n",
    "             \n",
    "             \n",
    "# if loading adversarial data, list the files here\n",
    "datafiles += [(os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"test1 eps_0.005 attack_fgsm.txt\"),\n",
    "                  \"ad_test1\", False),\n",
    "                 (os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"test2 eps_0.005 attack_fgsm.txt\"),\n",
    "                  \"ad_test2\", False),\n",
    "                 (os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"train1 eps_0.05 attack_fgsm.txt\"),\n",
    "                  \"ad_train1\", False),\n",
    "                 (os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"train2 eps_0.05 attack_fgsm.txt\"),\n",
    "                  \"ad_train2\", False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences dropped from ./data/test1.txt: 0 out of 76392 total\n",
      "Number of sentences dropped from ./data/test0.txt: 5 out of 50278 total\n",
      "Number of sentences dropped from ./data/train1.txt: 1 out of 267314 total\n",
      "Number of sentences dropped from ./data/train2.txt: 4 out of 176787 total\n",
      "Number of sentences dropped from ./adversarial_data/fgsm_eps_005/test1 eps_0.005 attack_fgsm.txt: 0 out of 7432 total\n",
      "Number of sentences dropped from ./adversarial_data/fgsm_eps_005/test2 eps_0.005 attack_fgsm.txt: 1 out of 7227 total\n",
      "Vocabulary Size: 8004\n",
      "763 batches\n",
      "502 batches\n",
      "4176 batches\n",
      "2762 batches\n",
      "74 batches\n",
      "72 batches\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# if vocabdict exists, load it\n",
    "if os.path.exists(os.path.join(OUTF, 'vocab.json')):\n",
    "    with open(os.path.join(OUTF, 'vocab.json')) as f:\n",
    "        vocabdict = json.load(f)\n",
    "else:\n",
    "    vocabdict = None\n",
    "\n",
    "\n",
    "label_ids = {\"pos\": 1, \"neg\": 0}\n",
    "id2label = {1:\"pos\", 0:\"neg\"}\n",
    "\n",
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [(os.path.join(DATA_PATH, \"test1.txt\"), \"test1\", False),\n",
    "             (os.path.join(DATA_PATH, \"test0.txt\"), \"test2\", False),\n",
    "             (os.path.join(DATA_PATH, \"train1.txt\"), \"train1\", True),\n",
    "             (os.path.join(DATA_PATH, \"train2.txt\"), \"train2\", True)]\n",
    "             \n",
    "# if loading adversarial data, this is the directory of the adv. dataset\n",
    "ADV_SUBDIRECTORY = 'fgsm_eps_005'\n",
    "    \n",
    "# if loading adversarial data, list the files here\n",
    "datafiles += [(os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"test1 eps_0.005 attack_fgsm.txt\"),\n",
    "                  \"ad_test1\", False),\n",
    "                 (os.path.join(ADVERSARIAL_DATA_PATH,\n",
    "                               ADV_SUBDIRECTORY,\n",
    "                               \"test2 eps_0.005 attack_fgsm.txt\"),\n",
    "                  \"ad_test2\", False)]\n",
    "\n",
    "corpus = Corpus(datafiles,\n",
    "                maxlen=MAXLEN,\n",
    "                vocab_size=VOCAB_SIZE,\n",
    "                lowercase=LOWERCASE,\n",
    "                vocab=vocabdict)\n",
    "\n",
    "# dumping vocabulary\n",
    "with open('{}/vocab.json'.format(OUTF), 'w') as f:\n",
    "    json.dump(corpus.dictionary.word2idx, f)\n",
    "\n",
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "\n",
    "eval_batch_size = 100\n",
    "test1_data = batchify(corpus.data['test1'], eval_batch_size, shuffle=False)\n",
    "test2_data = batchify(corpus.data['test2'], eval_batch_size, shuffle=False)\n",
    "train1_data = batchify(corpus.data['train1'], BATCH_SIZE, shuffle=False)\n",
    "train2_data = batchify(corpus.data['train2'], BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# if loading an adversarial dataset:\n",
    "adtest1_data = batchify(corpus.data['ad_test1'], eval_batch_size, shuffle=False)\n",
    "adtest2_data = batchify(corpus.data['ad_test2'], eval_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the models\n",
    "\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq2Decoder(emsize=EMSIZE,\n",
    "                      nhidden=NHIDDEN,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=NLAYERS,\n",
    "                      noise_r=NOISE_R,\n",
    "                      hidden_init=HIDDEN_INIT,\n",
    "                      dropout=DROPOUT,\n",
    "                      gpu=CUDA)\n",
    "\n",
    "gan_gen = MLP_G(ninput=Z_SIZE, noutput=NHIDDEN, layers=ARCH_G)\n",
    "gan_disc = MLP_D(ninput=NHIDDEN, noutput=1, layers=ARCH_D)\n",
    "classifier = MLP_Classify(ninput=NHIDDEN, noutput=1, layers=ARCH_CLASSIFY)\n",
    "g_factor = None\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "print(classifier)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=LR_AE)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=LR_GAN_G,\n",
    "                             betas=(BETA1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=LR_GAN_D,\n",
    "                             betas=(BETA1, 0.999))\n",
    "#### classify\n",
    "optimizer_classify = optim.Adam(classifier.parameters(),\n",
    "                                lr=LR_CLASSIFY,\n",
    "                                betas=(BETA1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if CUDA:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for byte-pair encoding\n",
    "BPE_VOCAB_SIZE=25000\n",
    "BPE_DIM=300\n",
    "bpemb_en = BPEmb(lang=\"en\", vs=BPE_VOCAB_SIZE, dim=BPE_DIM)\n",
    "\n",
    "def recover_original_sentence(source_batch):\n",
    "    \"\"\"\n",
    "    Given a sequence of tokens, recover the original text sentence.\n",
    "    \n",
    "    Returns:\n",
    "        List of sentences (strings)\n",
    "    \"\"\"\n",
    "    original_sentences = []\n",
    "    indices = source_batch.numpy()\n",
    "    for idx in indices:\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx if x > 1]\n",
    "        original_sentences.append(bpemb_en.decode(words))\n",
    "    return original_sentences\n",
    "\n",
    "def generate_sentences_from_embedding(whichclass, embedded_sentences_batch, sample=False):\n",
    "    \"\"\"\n",
    "    Given a sentence embedding, decode into text.\n",
    "    \n",
    "    Input:\n",
    "        sample: If False, perform greedy decoding. If True, sample from distribution.\n",
    "        \n",
    "    Returns:\n",
    "        List of sentences (strings)\n",
    "    \"\"\"\n",
    "    decoded_sentences = []\n",
    "    max_indices = autoencoder.generate(whichclass, embedded_sentences_batch, maxlen=50, sample=sample)\n",
    "    max_indices = max_indices.data.cpu().numpy()\n",
    "    for idx in max_indices:\n",
    "        # generated sentence\n",
    "        words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "        # truncate sentences to first occurrence of <eos>\n",
    "        truncated_sent = []\n",
    "        for w in words:\n",
    "            if w != '<eos>':\n",
    "                truncated_sent.append(w)\n",
    "            else:\n",
    "                break\n",
    "        decoded_sentences.append(bpemb_en.decode(truncated_sent))\n",
    "    return decoded_sentences\n",
    "\n",
    "def load_model(modelf=MODELF):\n",
    "    \"\"\"Load weights for the classifier and autoencoder from the directory modelf\"\"\"\n",
    "    if os.path.exists('{}/autoencoder_model.pt'.format(modelf)):\n",
    "        autoencoder.load_state_dict(torch.load('{}/autoencoder_model.pt'.format(modelf)))\n",
    "    if os.path.exists('{}/classifier_model.pt'.format(modelf)):            \n",
    "        classifier.load_state_dict(torch.load('{}/classifier_model.pt'.format(modelf)))\n",
    "\n",
    "def fgsm_attack(sentence_embedding, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_embedding = sentence_embedding + epsilon*sign_data_grad\n",
    "    # Clip within normal range for embedding\n",
    "    perturbed_embedding = torch.clamp(perturbed_embedding, -0.34, 0.32)\n",
    "    return perturbed_embedding\n",
    "    \n",
    "def evaluate_classifier_both_labels(data_1, data_2,\n",
    "                                    outputs_batched=False,\n",
    "                                    perturb=None,\n",
    "                                    epsilon=0.0,\n",
    "                                    include_codes=False,\n",
    "                                    pgd_iters=40,\n",
    "                                    alpha=.0005,\n",
    "                                    sample=False):\n",
    "    \"\"\"Evaluate the classifier for both labels\"\"\"\n",
    "    output_1 = evaluate_classifier(1, data_1,\n",
    "                                   outputs_batched=outputs_batched,\n",
    "                                   perturb=perturb,\n",
    "                                   epsilon=epsilon,\n",
    "                                   include_codes=include_codes,\n",
    "                                   pgd_iters=pgd_iters,\n",
    "                                   alpha=alpha,\n",
    "                                   sample=sample)\n",
    "    output_2 = evaluate_classifier(2, data_2,\n",
    "                                   outputs_batched=outputs_batched,\n",
    "                                   perturb=perturb,\n",
    "                                   epsilon=epsilon,\n",
    "                                   include_codes=include_codes,\n",
    "                                   pgd_iters=pgd_iters,\n",
    "                                   alpha=alpha,\n",
    "                                   sample=sample)\n",
    "\n",
    "    if include_codes:\n",
    "        loss_1, pred_1, labels_1, decoded_1, codes_1 = output_1\n",
    "        loss_2, pred_2, labels_2, decoded_2, codes_2 = output_2\n",
    "    else:\n",
    "        loss_1, pred_1, labels_1, decoded_1 = output_1\n",
    "        loss_2, pred_2, labels_2, decoded_2 = output_2\n",
    "    \n",
    "    labels = torch.cat((labels_1, labels_2))\n",
    "    pred = torch.cat((pred_1, pred_2))\n",
    "    acc = pred.eq(labels).float().mean().item()\n",
    "    \n",
    "    if include_codes:\n",
    "        return acc, pred, labels, decoded_1, decoded_2, np.vstack([np.vstack(codes_1), np.vstack(codes_2)])\n",
    "    else:\n",
    "        return acc, pred, labels, decoded_1, decoded_2\n",
    "        \n",
    "\n",
    "def evaluate_classifier(whichclass, data_source,\n",
    "                        outputs_batched=False,\n",
    "                        perturb=None,\n",
    "                        epsilon=0.0,\n",
    "                        include_codes=False,\n",
    "                        pgd_iters=40,\n",
    "                        alpha=.0005,\n",
    "                        sample=False):\n",
    "    \"\"\"\n",
    "    Evaluate a classifier with data corresponding to a single class.\n",
    "\n",
    "    input:\n",
    "        perturb: None, 'pgd', or 'fgsm'\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    source_sents = []\n",
    "    decoded_sents = []\n",
    "    if include_codes:\n",
    "        all_codes = []\n",
    "\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(CUDA, Variable(source))\n",
    "        labels = to_gpu(CUDA, Variable(torch.zeros(source.size(0)).fill_(whichclass-1)))\n",
    "        \n",
    "        code = autoencoder(0, source, lengths, noise=False, encode_only=True).detach()\n",
    "        \n",
    "        if include_codes:  # return codes for inspection\n",
    "            all_codes.append(code.cpu().numpy())\n",
    "\n",
    "        code.requires_grad = True\n",
    "        scores = classifier(code)\n",
    "        classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "        \n",
    "        if perturb == 'fgsm':\n",
    "            classifier.zero_grad()\n",
    "            classify_loss.backward()\n",
    "            code_grad = code.grad.data\n",
    "            perturbed_code = fgsm_attack(code, epsilon, code_grad)\n",
    "            \n",
    "            # decode embedded sentence and convert to words\n",
    "            decoded_sents.extend(generate_sentences_from_embedding(whichclass, perturbed_code, sample=sample))\n",
    "            \n",
    "            scores = classifier(perturbed_code)\n",
    "            classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "        elif perturb == 'pgd':\n",
    "            # alpha: step size\n",
    "            # epsilon: max perturbation (ball)\n",
    "            perturbed_code = code.clone().detach()\n",
    "            for i in range(pgd_iters):\n",
    "                perturbed_code.requires_grad = True\n",
    "                scores = classifier(perturbed_code)\n",
    "                tmp_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "                classifier.zero_grad()\n",
    "                tmp_loss.backward(retain_graph=True)\n",
    "\n",
    "                # step in the direction of the gradient\n",
    "                perturbed_code = perturbed_code + alpha * perturbed_code.grad.sign()\n",
    "                \n",
    "                # Workaround as PyTorch doesn't have elementwise clip\n",
    "                # from: https://gist.github.com/oscarknagg/45b187c236c6262b1c4bbe2d0920ded6#file-projected_gradient_descent-py\n",
    "                perturbed_code = torch.max(torch.min(perturbed_code, code + epsilon), code - epsilon).detach()\n",
    "                perturbed_code = torch.clamp(perturbed_code, -0.34, 0.32)\n",
    "                \n",
    "            # decode embedded sentence and convert to words\n",
    "            decoded_sents.extend(generate_sentences_from_embedding(whichclass, perturbed_code))\n",
    "            \n",
    "            scores = classifier(perturbed_code)\n",
    "            classify_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "\n",
    "        else:\n",
    "            # decode embedded sentence and convert to words\n",
    "            decoded_sents.extend(generate_sentences_from_embedding(whichclass, code))\n",
    "            \n",
    "\n",
    "        pred = scores.data.round().squeeze(1)\n",
    "        total_loss += classify_loss.cpu().item()\n",
    "\n",
    "        all_pred.append(pred)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    if outputs_batched == False:\n",
    "        if include_codes:\n",
    "            return total_loss/len(data_source), torch.cat(all_pred), torch.cat(all_labels), decoded_sents, all_codes\n",
    "        else:\n",
    "            return total_loss/len(data_source), torch.cat(all_pred), torch.cat(all_labels), decoded_sents\n",
    "        \n",
    "    if include_codes:\n",
    "        return total_loss/len(data_source), all_pred, all_labels, decoded_sents, all_codes\n",
    "    else:\n",
    "        return total_loss/len(data_source), all_pred, all_labels, decoded_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White-Box Setting: Evaluate Under FGSM and PGD Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the following models\n",
    "models = [\n",
    "    \"model_output_0.0005_adtrain_noclip\", \n",
    "    \"model_output_0.001_adtrain_noclip\", \n",
    "    \"model_output_0.05_adtrain/20 epochs\",\n",
    "    \"model_output_0.25_adtrain/20 epochs\",\n",
    "    \"model_output_pgd_40_0.005_0.0005\",\n",
    "    \"model_output_128_128_128_NoBatchNorm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: model_output_0.0005_adtrain_noclip\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 0.0\n",
      "Loaded model: model_output_0.001_adtrain_noclip\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 6.324111018329859e-05\n",
      "Loaded model: model_output_0.05_adtrain/20 epochs\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 0.9255415797233582\n",
      "Loaded model: model_output_0.25_adtrain/20 epochs\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 0.0015415020752698183\n",
      "Loaded model: model_output_pgd_40_0.005_0.0005\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 0.0835968405008316\n",
      "Loaded model: model_output_128_128_128_NoBatchNorm\n",
      "\n",
      "Epsilon: 0.02\n",
      "Accuracy: Adversarial dataset - Test set 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate under FGSM attack\n",
    "for modelf in models:\n",
    "    load_model(modelf=modelf)\n",
    "    print(\"Loaded model:\", modelf)\n",
    "    for epsilon in [0.0005, 0.001, .005, .01, 0.05, 0.1, 0.25, 0.5]:\n",
    "        print('Epsilon:', epsilon)\n",
    "        test_acc, _, _, _, _ = evaluate_classifier_both_labels(test1_data, test2_data,\n",
    "                                                               perturb='fgsm',\n",
    "                                                               epsilon=epsilon,\n",
    "                                                               include_codes=False)\n",
    "        print('Accuracy: Adversarial dataset - Test set {}'.format(test_acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: model_output_0.0005_adtrain_noclip\n",
      "0.005 0.0005 1\n",
      "Accuracy: Adversarial dataset - Train set 0.8510534763336182\n",
      "Accuracy: Adversarial dataset - Test set 0.8369882106781006\n",
      "\n",
      "0.005 0.0005 2\n",
      "Accuracy: Adversarial dataset - Train set 0.7692170739173889\n",
      "Accuracy: Adversarial dataset - Test set 0.7545692324638367\n",
      "\n",
      "0.005 0.0005 5\n",
      "Accuracy: Adversarial dataset - Train set 0.4109365940093994\n",
      "Accuracy: Adversarial dataset - Test set 0.4018498361110687\n",
      "\n",
      "0.005 0.0005 10\n",
      "Accuracy: Adversarial dataset - Train set 0.03746351599693298\n",
      "Accuracy: Adversarial dataset - Test set 0.038339924067258835\n",
      "\n",
      "0.005 0.0005 20\n",
      "Accuracy: Adversarial dataset - Train set 0.0297478549182415\n",
      "Accuracy: Adversarial dataset - Test set 0.030774705111980438\n",
      "\n",
      "0.005 0.0005 40\n",
      "Accuracy: Adversarial dataset - Train set 0.028464164584875107\n",
      "Accuracy: Adversarial dataset - Test set 0.029343875125050545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate under PGD attack\n",
    "\n",
    "modelf = \"model_output_0.0005_adtrain_noclip\"\n",
    "load_model(modelf=modelf)\n",
    "print(\"Loaded model:\", modelf)\n",
    "\n",
    "# attack configurations\n",
    "configs = [(0.005, 0.0005, 1),\n",
    "           (0.005, 0.0005, 2),\n",
    "           (0.005, 0.0005, 5),\n",
    "           (0.005, 0.0005, 10),\n",
    "           (0.005, 0.0005, 20),\n",
    "           (0.005, 0.0005, 40)\n",
    "]\n",
    "\n",
    "for epsilon, alpha, iters in configs:\n",
    "    print(\"{} {} {}\".format(epsilon, alpha, iters))\n",
    "    test_acc, _, _, _, _ = evaluate_classifier_both_labels(test1_data, test2_data,\n",
    "                                                           perturb='pgd',\n",
    "                                                           epsilon=epsilon,\n",
    "                                                           alpha=alpha,\n",
    "                                                           include_codes=False,\n",
    "                                                           pgd_iters=iters)\n",
    "    print('Accuracy: Adversarial dataset - Test set {}'.format(test_acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a decoded adversarial dataset for black-box setting experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_adversarial_dataset(data1, data2,\n",
    "                               phase='train',\n",
    "                               perturb=None, epsilon=0, alpha=.0005, pgd_iters=40,  # perturbation configuration\n",
    "                               directory='./adversarial_data/',\n",
    "                               sample=False # for decoding\n",
    "                              ):\n",
    "    _, pred_1, labels_1, decoded_1 = evaluate_classifier(1, data1, outputs_batched=True, perturb=None)\n",
    "    _, pred_2, labels_2, decoded_2 = evaluate_classifier(2, data2, outputs_batched=True, perturb=None)\n",
    "\n",
    "    # check which examples are classified correctly\n",
    "    data1_correct = []\n",
    "    data2_correct = []\n",
    "\n",
    "    for i, data in enumerate(data1):\n",
    "        batch_pred = pred_1[i].cpu().numpy()\n",
    "        batch_labels = labels_1[i].cpu().numpy()\n",
    "\n",
    "        correctly_classified_indices = np.where(batch_pred == batch_labels)[0]\n",
    "        source, target, length = data\n",
    "        source_correct = source[correctly_classified_indices]\n",
    "        target_correct = target[correctly_classified_indices]\n",
    "        length_correct = [length[i] for i in correctly_classified_indices]\n",
    "        data1_correct.append((source_correct, target_correct, length_correct))\n",
    "\n",
    "    for i, data in enumerate(data2):\n",
    "        batch_pred = pred_2[i].cpu().numpy()\n",
    "        batch_labels = labels_2[i].cpu().numpy()\n",
    "\n",
    "        correctly_classified_indices = np.where(batch_pred == batch_labels)[0]\n",
    "        source, target, length = data\n",
    "\n",
    "        source_correct = source[correctly_classified_indices]\n",
    "        target_correct = target[correctly_classified_indices]\n",
    "        length_correct = [length[i] for i in correctly_classified_indices]\n",
    "        data2_correct.append((source_correct, target_correct, length_correct))\n",
    "\n",
    "    # generate adversarial examples for the correctly classified examples\n",
    "    _, _, _, adv_decoded_1, adv_decoded_2 = evaluate_classifier_both_labels(data1_correct, data2_correct,\n",
    "                                                                            outputs_batched=False,\n",
    "                                                                            perturb=perturb,\n",
    "                                                                            epsilon=epsilon,\n",
    "                                                                            alpha=alpha,\n",
    "                                                                            pgd_iters=pgd_iters,\n",
    "                                                                            sample=sample)\n",
    "    \n",
    "    # also decode the original sentences for comparison\n",
    "    _, _, _, source_decoded_1, source_decoded_2 = evaluate_classifier_both_labels(data1_correct, data2_correct,\n",
    "                                                                                  outputs_batched=False,\n",
    "                                                                                  perturb=None,\n",
    "                                                                                  sample=sample)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "    # Create new datasets of decoded adversarial sentences, but only those that \n",
    "    # do not exactly match their unperturbed counterpart after both are decoded\n",
    "    with open(directory + '{}1 eps_{} attack_{}.txt'.format(phase, epsilon, perturb), 'w') as f:\n",
    "        for s, d in zip(source_decoded_1, adv_decoded_1):\n",
    "            if s != d:\n",
    "                f.write(\"%s\\n\" % d)\n",
    "\n",
    "    with open(directory + '{}2 eps_{} attack_{}.txt'.format(phase, epsilon, perturb), 'w') as f:\n",
    "        for s, d in zip(source_decoded_2, adv_decoded_2):\n",
    "            if s != d:\n",
    "                f.write(\"%s\\n\" % d)\n",
    "\n",
    "    # For inspection, write all of the data to file\n",
    "    # (for examples that were initially classified correctly by the classifier)\n",
    "    original_sentences_1 = []\n",
    "    for s, t, l in data1_correct:\n",
    "        original_sentences_1.extend(recover_original_sentence(s))\n",
    "        \n",
    "    original_sentences_2 = []\n",
    "    for s, t, l in data2_correct:\n",
    "        original_sentences_2.extend(recover_original_sentence(s))\n",
    "\n",
    "    df_1 = pd.DataFrame(list(zip(original_sentences_1,\n",
    "                                 adv_decoded_1,\n",
    "                                 source_decoded_1)), \n",
    "                   columns =['original', 'adv_decoded', 'original_decoded']) \n",
    "    df_1['label'] = 1\n",
    "\n",
    "    df_2 = pd.DataFrame(list(zip(original_sentences_2,\n",
    "                                 adv_decoded_2,\n",
    "                                 source_decoded_2)), \n",
    "                   columns =['original', 'adv_decoded', 'original_decoded']) \n",
    "    df_2['label'] = 2\n",
    "\n",
    "    df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "    df['included_in_adv_dataset'] = df['adv_decoded'] != df['original_decoded']\n",
    "    df.to_csv(directory + 'inspect {} eps_{} attack_{}.csv'.format(phase, epsilon, perturb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbations will be applied to embedded sentences using this model:\n",
    "load_model(\"model_output_128_128_128_NoBatchNorm\")\n",
    "\n",
    "create_adversarial_dataset(test1_data, test2_data,  # datasets to be perturbed\n",
    "                           'test',\n",
    "                           perturb='pgd',  # perturbation configuration\n",
    "                           epsilon=0.5,\n",
    "                           alpha=0.05,\n",
    "                           pgd_iters=20,\n",
    "                           directory='./adversarial_data/pgd_eps_5_alpa_05_iters_20/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-Box Setting: Evaluate models against a decoded adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: model_output_0.0005_adtrain_noclip\n",
      "Accuracy: Adversarial dataset - Test set 0.89693284034729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the following models\n",
    "models = [\n",
    "    \"model_output_0.0005_adtrain_noclip\", \n",
    "    \"model_output_0.001_adtrain_noclip\",\n",
    "    \"model_output_0.05_adtrain/20 epochs\",\n",
    "    \"model_output_0.25_adtrain/20 epochs\",\n",
    "    \"model_output_pgd_40_0.005_0.0005\",\n",
    "    \"model_output_128_128_128_NoBatchNorm\"]\n",
    "\n",
    "for modelf in models:\n",
    "    load_model(modelf=modelf)\n",
    "    print(\"Loaded model:\", modelf)\n",
    "    \n",
    "    # evaluate the classifier against the adversarial dataset\n",
    "    test_acc, pred, labels, _, _ = evaluate_classifier_both_labels(adtest1_data, adtest2_data,\n",
    "                                                                   outputs_batched=False,\n",
    "                                                                   perturb=None)\n",
    "    print('Accuracy: Adversarial dataset - Test set {}'.format(test_acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further inspection, keep only the adv. examples that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences_1 = []\n",
    "for s, t, l in adtest1_data:\n",
    "    original_sentences_1.extend(recover_original_sentence(s))\n",
    "\n",
    "original_sentences_2 = []\n",
    "for s, t, l in adtest2_data:\n",
    "    original_sentences_2.extend(recover_original_sentence(s))\n",
    "    \n",
    "sentences = np.array(original_sentences_1 + original_sentences_2)\n",
    "df = pd.DataFrame([sentences, labels.cpu().numpy(), pred.cpu().numpy()]).T\n",
    "df.columns = ['original_sentence', 'label', 'pred']\n",
    "\n",
    "with open('./adversarial_data/pgd_eps_05_alpha_005_iters_20_only_misclassified_decoded/test1 eps_0.05 attack_pgd.txt', 'w') as f:\n",
    "    for d in df[(df.label != df.pred) & (df.label == 0)]['original_sentence']:\n",
    "            f.write(\"%s\\n\" % d)\n",
    "\n",
    "with open('./adversarial_data/pgd_eps_05_alpha_005_iters_20_only_misclassified_decoded/test2 eps_0.05 attack_pgd.txt', 'w') as f:\n",
    "    for d in df[(df.label != df.pred) & (df.label == 1)]['original_sentence']:\n",
    "            f.write(\"%s\\n\" % d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the distributions of the embedded sentences\n",
    "We examine the distribution of embedded sentences before perturbation to understand the range of realistic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(512):\n",
    "    pd.Series(codes[:, i]).plot.hist(alpha=0.1, bins=np.arange(-.5, .5, .01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(512):\n",
    "    pd.Series(codes[:, i]).plot.hist(alpha=0.1, bins=np.arange(-.5, .5, .01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "Calculate the perplexity of our original and perturbed sentences against a pretrained LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up gpu\n",
    "del autoencoder\n",
    "del classifier\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "lm = lm.cuda()\n",
    "lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    with torch.no_grad():\n",
    "        tensor_input = tensor_input.cuda()\n",
    "        outputs = lm(tensor_input, labels=tensor_input)\n",
    "    loss = outputs[0]  \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(sentences):\n",
    "    \"\"\"\n",
    "    Assume sentences is a list of strings (space delimited sentences)\n",
    "    \"\"\"\n",
    "    total_nll = 0\n",
    "    total_wc = 0\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        sent_score = score(sent)\n",
    "        word_count = len(words)\n",
    "        total_wc += word_count\n",
    "        total_nll += sent_score\n",
    "    ppl = math.exp(total_nll/total_wc)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7038252596041783"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ppl(np.array(results['source'])[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7774270590392736"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices of adversarial examples\n",
    "e = 0.0015\n",
    "idx = np.logical_and(\n",
    "    np.array(results['source']) != np.array(results[e]['decoded']),\n",
    "    ~results[e]['labels'].eq(results[e]['pred']).data.cpu().numpy())\n",
    "\n",
    "get_ppl(np.array(results[e]['decoded'])[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
